{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.10.0"},
  "colab": {"provenance": []}
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "md-title",
   "metadata": {},
   "source": "# 03 — Evaluation\n\nEvaluates all model variants and compares against paper baselines.\n\n**Metrics computed:**\n- BLEU-1/2/3/4 (word-level, correct tokenisation)\n- BLEU-1/2/3/4 (char-level, paper's original method — for direct comparison)\n- Token F1\n- METEOR\n- ROUGE-L\n- Perplexity (T5 models only)\n\n**Models evaluated:**\n- `t5:baseline`, `t5:topic`, `t5:topic2x` — auto-discovered from `models/`\n- Ollama models — auto-discovered from running local server\n- Gemini models — configured in `config/pipeline.yaml`\n\n> Prerequisite: `01_data_generation.ipynb` (MixKhanQ data.csv must exist)."
  },
  {
   "cell_type": "markdown",
   "id": "md-setup",
   "metadata": {},
   "source": "## Setup"
  },
  {
   "cell_type": "code",
   "id": "code-env",
   "metadata": {},
   "source": "import sys\nfrom pathlib import Path\n\nIN_COLAB = 'google.colab' in sys.modules\n\nif IN_COLAB:\n    REPO_URL = \"https://github.com/YOUR_ORG/YOUR_REPO.git\"  # TODO: set your URL\n    !git clone {REPO_URL} /content/ai4ed-qg -q\n    %cd /content/ai4ed-qg\n    !pip install -q torch transformers datasets sentence-transformers \\\n                    evaluate rouge_score nltk sentencepiece pyyaml \\\n                    tqdm pandas python-dotenv google-genai\n\n    import nltk\n    for res in ('punkt', 'punkt_tab', 'wordnet', 'omw-1.4'):\n        nltk.download(res, quiet=True)\n\n    from google.colab import drive\n    drive.mount('/content/drive')\n    DRIVE_DIR = Path('/content/drive/MyDrive/ai4ed_qg')\n\n    # Restore models and data from Drive\n    import shutil\n    for subdir in ('models', 'data/training'):\n        src = DRIVE_DIR / subdir.split('/')[-1]\n        dst = Path('/content/ai4ed-qg') / subdir\n        if src.exists():\n            dst.parent.mkdir(parents=True, exist_ok=True)\n            shutil.copytree(src, dst, dirs_exist_ok=True)\n            print(f\"Restored {subdir}/ from Drive\")\nelse:\n    DRIVE_DIR = None\n\nimport os\nproject_root = Path('/content/ai4ed-qg') if IN_COLAB else Path.cwd()\nif project_root.name == 'notebooks':\n    project_root = project_root.parent\nos.chdir(project_root)\nsys.path.insert(0, str(project_root))\nprint(f\"Working dir: {os.getcwd()}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "md-apikeys",
   "metadata": {},
   "source": "## API Keys\n\nGemini evaluation requires `GOOGLE_API_KEY` (or `GEMINI_API_KEY`). Set it in Colab Secrets or your local `.env`.\n\nOllama models are auto-discovered — just have Ollama running locally."
  },
  {
   "cell_type": "code",
   "id": "code-apikeys",
   "metadata": {},
   "source": "if IN_COLAB:\n    from google.colab import userdata\n    import os\n    for key_name in ('GOOGLE_API_KEY', 'GEMINI_API_KEY'):\n        try:\n            os.environ[key_name] = userdata.get(key_name)\n            print(f\"{key_name} loaded from Colab Secrets\")\n            break\n        except Exception:\n            pass\nelse:\n    try:\n        from dotenv import load_dotenv\n        load_dotenv()\n    except ImportError:\n        pass\n\napi_key = os.environ.get('GOOGLE_API_KEY') or os.environ.get('GEMINI_API_KEY', '')\nprint(f\"Gemini API key: {'set (' + api_key[:6] + '...)' if api_key else 'not set (Gemini models will be skipped)'}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "md-init",
   "metadata": {},
   "source": "## Initialise Pipeline"
  },
  {
   "cell_type": "code",
   "id": "code-init",
   "metadata": {},
   "source": "from src.pipeline import Pipeline\n\npipe = Pipeline('config/pipeline.yaml')\npipe.status()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "md-evaluate",
   "metadata": {},
   "source": "## Evaluate — KhanQ\n\nEvaluates all available models on the MixKhanQ test set (653 pairs) and prints a comparison table that includes paper baselines.\n\n> The table shows both word-level BLEU (correct) and char-level BLEU (paper's original buggy method) for direct comparison with the paper's Table 2 numbers."
  },
  {
   "cell_type": "code",
   "id": "code-eval-khanq",
   "metadata": {},
   "source": "# Evaluate all auto-discovered models on KhanQ\n# models='all'  → T5 checkpoints (disk) + Ollama (live server) + Gemini (config)\n# models='t5:topic,t5:baseline' → evaluate specific models only\n\nresults = pipe.evaluate(\n    models='all',\n    dataset='khanq',\n    # output_dir='results/my_eval',  # optional: pin the output directory\n)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "md-eval-squad",
   "metadata": {},
   "source": "## Evaluate — SQuAD (optional)\n\nEvaluates on the SQuAD test split. Only relevant if you ran the full SQuAD data pipeline."
  },
  {
   "cell_type": "code",
   "id": "code-eval-squad",
   "metadata": {},
   "source": "# Uncomment to also evaluate on SQuAD:\n# results_squad = pipe.evaluate(models='all', dataset='squad')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "md-results",
   "metadata": {},
   "source": "## Inspect Results"
  },
  {
   "cell_type": "code",
   "id": "code-results-table",
   "metadata": {},
   "source": "import pandas as pd\n\n# Build a tidy DataFrame from the results dict\nrows = []\nfor key, m in results.items():\n    rows.append({\n        'model':      key,\n        'n':          m.get('num_samples', '-'),\n        'B1':         round(m.get('bleu1', 0), 3),\n        'B2':         round(m.get('bleu2', 0), 3),\n        'B3':         round(m.get('bleu3', 0), 3),\n        'B4':         round(m.get('bleu4', 0), 3),\n        'B1c (paper)': round(m.get('bleu1_char', 0), 3),\n        'B4c (paper)': round(m.get('bleu4_char', 0), 3),\n        'F1':         round(m.get('f1', 0), 3),\n        'METEOR':     round(m.get('meteor', 0), 3),\n        'ROUGE-L':    round(m.get('rouge_l', 0), 3),\n        'PPL':        round(m.get('perplexity', float('nan')), 3),\n    })\n\ndf = pd.DataFrame(rows).set_index('model')\npd.set_option('display.max_columns', None)\npd.set_option('display.width', 140)\ndf",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "code-best-model",
   "metadata": {},
   "source": "# Find best model by word-level BLEU-4\nif 'B4' in df.columns and df['B4'].notna().any():\n    best = df['B4'].idxmax()\n    print(f\"Best model by BLEU-4 (word-level): {best}\")\n    print(df.loc[best])",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "md-examples",
   "metadata": {},
   "source": "## Qualitative Examples\n\nCompare what different models generate for the same input."
  },
  {
   "cell_type": "code",
   "id": "code-examples",
   "metadata": {},
   "source": "import random\n\n# Pick any model that has predictions stored\nsample_key = next(\n    (k for k, v in results.items() if v.get('predictions')),\n    None\n)\n\nif sample_key:\n    preds = results[sample_key]['predictions']\n    refs  = results[sample_key]['references']\n    indices = random.sample(range(len(preds)), min(5, len(preds)))\n\n    print(f\"Sample predictions from: {sample_key}\")\n    print(\"=\" * 70)\n    for i in indices:\n        print(f\"\\n[{i}] Reference : {refs[i]}\")\n        print(f\"     Prediction: {preds[i]}\")\nelse:\n    print(\"No predictions stored (check results dict)\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "md-generate",
   "metadata": {},
   "source": "## Single Question Generation\n\nInteractively generate a question from any topic + context."
  },
  {
   "cell_type": "code",
   "id": "code-generate",
   "metadata": {},
   "source": "topic = \"Electronegativity\"\ncontext = (\n    \"Electronegativity is a measure of the tendency of an atom to attract \"\n    \"a bonding pair of electrons. The Pauling scale is the most commonly \"\n    \"used scale for electronegativity. Fluorine is the most electronegative \"\n    \"element and is assigned a value of 4.0 on the Pauling scale.\"\n)\n\n# T5 fine-tuned model\ntry:\n    q_t5 = pipe.generate(topic=topic, context=context, mode='topic')\n    print(f\"T5 (topic)  : {q_t5}\")\nexcept FileNotFoundError:\n    print(\"T5 model not trained yet — run 02_distillation_training.ipynb first\")\n\n# Gemini zero-shot (requires GOOGLE_API_KEY)\n# from src.evaluation.models import GeminiBaseline\n# gem = GeminiBaseline('gemini-2.5-flash')\n# print(f\"Gemini flash: {gem.generate_question(topic, context)}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "md-save",
   "metadata": {},
   "source": "## Save Results (Colab)"
  },
  {
   "cell_type": "code",
   "id": "code-save",
   "metadata": {},
   "source": "if IN_COLAB and DRIVE_DIR:\n    import shutil\n    src = project_root / 'results'\n    dst = DRIVE_DIR / 'results'\n    shutil.copytree(src, dst, dirs_exist_ok=True)\n    print(f\"Results synced to {dst}\")",
   "outputs": [],
   "execution_count": null
  }
 ]
}
