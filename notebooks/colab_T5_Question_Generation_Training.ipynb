{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "id": "md-title",
      "metadata": {
        "id": "md-title"
      },
      "source": [
        "# T5 Question Generation — Colab Training\n",
        "\n",
        "Standalone notebook for training and evaluating T5 topic-controlled question generation on Google Colab.\n",
        "\n",
        "**Use this notebook when:**\n",
        "- You have already run the data pipeline locally (stages 1–4) and have the CSV files\n",
        "- You want to train on Colab's GPU and evaluate with the full metric suite\n",
        "\n",
        "**Steps:**\n",
        "1. Setup environment and clone repo\n",
        "2. Upload training CSVs from your local pipeline run\n",
        "3. Train one or more model variants with `pipe.train()`\n",
        "4. Evaluate with `pipe.evaluate()` against paper baselines\n",
        "5. Download the trained model\n",
        "\n",
        "**Expected CSV files** (produced by `01_data_generation.ipynb` or `pipeline.py dataset`):\n",
        "```\n",
        "data/training/squad/baseline/  train.csv  val.csv  test.csv\n",
        "data/training/squad/mixsquad/  train.csv  val.csv  test.csv\n",
        "data/training/khanq/mixkhanq/  data.csv\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VXAIerRedBtn"
      },
      "id": "VXAIerRedBtn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "md-setup",
      "metadata": {
        "id": "md-setup"
      },
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "id": "code-gpu",
      "metadata": {
        "id": "code-gpu",
        "outputId": "b6b5bc71-0cce-42f3-eb1b-80cf4c159127",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Check GPU\n",
        "!nvidia-smi\n",
        "\n",
        "import torch\n",
        "print(f\"\\nPyTorch : {torch.__version__}\")\n",
        "print(f\"CUDA    : {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    name = torch.cuda.get_device_name(0)\n",
        "    mem  = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"GPU     : {name} ({mem:.0f} GB)\")\n",
        "    # Suggest batch size based on available VRAM\n",
        "    suggested_batch = 128 if mem >= 35 else 64 if mem >= 15 else 32\n",
        "    print(f\"Suggested batch size: {suggested_batch}\")\n",
        "else:\n",
        "    print(\"WARNING: No GPU detected. Training will be very slow.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Feb 23 17:02:53 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 580.82.07              Driver Version: 580.82.07      CUDA Version: 13.0     |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "\n",
            "PyTorch : 2.10.0+cu128\n",
            "CUDA    : True\n",
            "GPU     : Tesla T4 (16 GB)\n",
            "Suggested batch size: 64\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "id": "code-env",
      "metadata": {
        "id": "code-env",
        "outputId": "25d1853b-7a69-4f77-a82b-c85357403b53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import sys, os\n",
        "from pathlib import Path\n",
        "\n",
        "# ── Clone repository ──────────────────────────────────────────────────────────\n",
        "# TODO: replace with your actual repository URL\n",
        "REPO_URL = \"https://github.com/Byambaa0325/question-generation-distillation.git\"\n",
        "!git clone {REPO_URL} /content/ai4ed-qg -q\n",
        "%cd /content/ai4ed-qg\n",
        "\n",
        "# ── Install dependencies ──────────────────────────────────────────────────────\n",
        "!pip install -q torch transformers datasets accelerate sentencepiece \\\n",
        "                evaluate rouge_score nltk sentence-transformers \\\n",
        "                pyyaml tqdm pandas python-dotenv\n",
        "\n",
        "import nltk\n",
        "for res in ('punkt', 'punkt_tab', 'wordnet', 'omw-1.4'):\n",
        "    nltk.download(res, quiet=True)\n",
        "\n",
        "sys.path.insert(0, '/content/ai4ed-qg')\n",
        "os.chdir('/content/ai4ed-qg')\n",
        "print(f\"Working dir: {os.getcwd()}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ai4ed-qg\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Working dir: /content/ai4ed-qg\n"
          ]
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "id": "code-drive",
      "metadata": {
        "id": "code-drive",
        "outputId": "59a47f8c-6a67-4ef6-f562-c5f290a0b40b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "source": [
        "from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "#DRIVE_DIR = Path('/content/drive/MyDrive/ai4ed_qg')\n",
        "DRIVE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "print(f\"Drive directory: {DRIVE_DIR}\")\n",
        "\n",
        "# Restore any previously saved models from Drive\n",
        "import shutil\n",
        "for subdir in ('models', 'results'):\n",
        "    src = DRIVE_DIR / subdir\n",
        "    dst = Path('/content/ai4ed-qg') / subdir\n",
        "    if src.exists():\n",
        "        shutil.copytree(src, dst, dirs_exist_ok=True)\n",
        "        print(f\"Restored {subdir}/ from Drive\")"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3566631633.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mDRIVE_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/ai4ed_qg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mDRIVE_DIR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "id": "md-data",
      "metadata": {
        "id": "md-data"
      },
      "source": [
        "## 2. Upload Training Data\n",
        "\n",
        "Upload the CSV files produced by the data pipeline. You need at minimum:\n",
        "- `train.csv` + `val.csv` for the mode you want to train\n",
        "- `data.csv` (MixKhanQ) for evaluation\n",
        "\n",
        "**Option A**: Upload from your local machine using the cell below.\n",
        "**Option B**: Copy from Drive if you already uploaded them."
      ]
    },
    {
      "cell_type": "code",
      "id": "code-upload",
      "metadata": {
        "id": "code-upload"
      },
      "source": [
        "# ── Option A: upload from local machine ──────────────────────────────────────\n",
        "# Run this cell and select your CSV files.\n",
        "# Files will be placed in the correct data/training/ subdirectory.\n",
        "\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "print(\"Select CSV files to upload (train.csv, val.csv, test.csv, data.csv)...\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "for filename in uploaded:\n",
        "    print(f\"Uploaded: {filename}\")\n",
        "\n",
        "# After uploading, place files manually:\n",
        "# data/training/squad/mixsquad/train.csv  → for 'topic' mode\n",
        "# data/training/squad/baseline/train.csv  → for 'baseline' mode\n",
        "# data/training/khanq/mixkhanq/data.csv   → for evaluation"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "code-place-files",
      "metadata": {
        "id": "code-place-files",
        "outputId": "2da89ef8-5497-4f42-ad17-b41bb4f1a0f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# ── Place uploaded files into the correct directories ─────────────────────────\n",
        "# Edit this mapping to match what you uploaded.\n",
        "# Keys are uploaded filenames, values are destination paths.\n",
        "\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "file_placement = {\n",
        "    'train.csv': 'data/training/squad/mixsquad/train.csv',\n",
        "    'val.csv':   'data/training/squad/mixsquad/val.csv',\n",
        "    'test.csv':  'data/training/squad/mixsquad/test.csv',\n",
        "    'data.csv':  'data/training/khanq/mixkhanq/data.csv',\n",
        "}\n",
        "\n",
        "for src_name, dst_rel in file_placement.items():\n",
        "    src = Path(src_name)\n",
        "    dst = Path(dst_rel)\n",
        "    if src.exists():\n",
        "        dst.parent.mkdir(parents=True, exist_ok=True)\n",
        "        shutil.copy(src, dst)\n",
        "        print(f\"Placed: {src_name} → {dst}\")\n",
        "    else:\n",
        "        print(f\"Not found: {src_name}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not found: train.csv\n",
            "Not found: val.csv\n",
            "Not found: test.csv\n",
            "Not found: data.csv\n"
          ]
        }
      ],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "id": "code-verify-data",
      "metadata": {
        "id": "code-verify-data"
      },
      "source": [
        "# ── Option B: copy from Drive ─────────────────────────────────────────────────\n",
        "# If you already have data on Drive, copy it here:\n",
        "import shutil\n",
        "for subdir in ('processed', 'training'):\n",
        "    src = DRIVE_DIR / subdir\n",
        "    dst = Path('/content/ai4ed-qg/data') / subdir\n",
        "    if src.exists():\n",
        "        shutil.copytree(src, dst, dirs_exist_ok=True)\n",
        "        print(f\"Restored data/{subdir}/ from Drive\")\n",
        "    else:\n",
        "        print(f\"Not found in Drive: {subdir}/\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "md-init",
      "metadata": {
        "id": "md-init"
      },
      "source": [
        "## 3. Initialise Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "id": "code-init",
      "metadata": {
        "id": "code-init",
        "outputId": "28254e7e-3fa5-4cad-9551-1de1b305aea6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from src.pipeline import Pipeline\n",
        "\n",
        "pipe = Pipeline('config/pipeline.yaml')\n",
        "pipe.status()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Pipeline status:\n",
            "  [-] convert.squad.text\n",
            "  [-] convert.squad.question\n",
            "  [-] convert.khanq.text\n",
            "  [-] convert.khanq.question\n",
            "  [-] wikify.squad.text\n",
            "  [-] wikify.squad.question\n",
            "  [-] wikify.khanq.text\n",
            "  [-] wikify.khanq.question\n",
            "  [-] topics.squad.enriched\n",
            "  [-] topics.squad.filtered\n",
            "  [-] topics.khanq.enriched\n",
            "  [-] topics.khanq.filtered\n",
            "  [-] dataset.squad.baseline\n",
            "  [-] dataset.squad.mixsquad\n",
            "  [-] dataset.squad.mixsquad2x\n",
            "  [-] dataset.khanq.baseline\n",
            "  [-] dataset.khanq.mixsquad\n",
            "  [-] dataset.khanq.mixsquad2x\n",
            "  [-] train.baseline\n",
            "  [-] train.topic\n",
            "  [-] train.topic2x\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'convert.squad.text': False,\n",
              " 'convert.squad.question': False,\n",
              " 'convert.khanq.text': False,\n",
              " 'convert.khanq.question': False,\n",
              " 'wikify.squad.text': False,\n",
              " 'wikify.squad.question': False,\n",
              " 'wikify.khanq.text': False,\n",
              " 'wikify.khanq.question': False,\n",
              " 'topics.squad.enriched': False,\n",
              " 'topics.squad.filtered': False,\n",
              " 'topics.khanq.enriched': False,\n",
              " 'topics.khanq.filtered': False,\n",
              " 'dataset.squad.baseline': False,\n",
              " 'dataset.squad.mixsquad': False,\n",
              " 'dataset.squad.mixsquad2x': False,\n",
              " 'dataset.khanq.baseline': False,\n",
              " 'dataset.khanq.mixsquad': False,\n",
              " 'dataset.khanq.mixsquad2x': False,\n",
              " 'train.baseline': False,\n",
              " 'train.topic': False,\n",
              " 'train.topic2x': False}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "id": "code-config",
      "metadata": {
        "id": "code-config",
        "outputId": "f601f2bf-cdc8-4dcb-cc6c-0b6f669a3b94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# ── Tweak training config for your GPU ───────────────────────────────────────\n",
        "# Edit pipeline.yaml to make changes permanent, or override here:\n",
        "\n",
        "pipe.config.training.batch  = 64     # 128 for A100, 64 for T4/V100, 32 if OOM\n",
        "pipe.config.training.epochs = 50     # paper uses 50\n",
        "pipe.config.training.lr     = 1e-3\n",
        "\n",
        "t = pipe.config.training\n",
        "print(f\"Model      : {t.model_name}\")\n",
        "print(f\"Batch size : {t.batch}\")\n",
        "print(f\"Epochs     : {t.epochs}\")\n",
        "print(f\"LR         : {t.lr}\")\n",
        "print(f\"Max input  : {t.max_input_len} tokens\")\n",
        "print(f\"Max output : {t.max_output_len} tokens\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model      : google-t5/t5-small\n",
            "Batch size : 64\n",
            "Epochs     : 50\n",
            "LR         : 0.001\n",
            "Max input  : 200 tokens\n",
            "Max output : 45 tokens\n"
          ]
        }
      ],
      "execution_count": 6
    },
    {
      "cell_type": "markdown",
      "id": "md-train",
      "metadata": {
        "id": "md-train"
      },
      "source": [
        "## 4. Train\n",
        "\n",
        "Train the model variant you need. The pipeline uses the correct paper format for all modes:\n",
        "```\n",
        "Input:  <topic> {topic} <context> {combined text}\n",
        "Target: {question}\n",
        "```\n",
        "\n",
        "Saved to `models/{mode}/best_model/` (best checkpoint by validation loss)."
      ]
    },
    {
      "cell_type": "code",
      "id": "code-train-topic",
      "metadata": {
        "id": "code-train-topic"
      },
      "source": [
        "# ── TopicQG — trained on MixSQuAD (10k mixed pairs) ─────────────────────────\n",
        "model_path = pipe.train(mode='topic', dataset='squad')\n",
        "print(f\"\\nModel saved to: {model_path}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "code-train-baseline",
      "metadata": {
        "id": "code-train-baseline"
      },
      "source": [
        "# ── Baseline — context only, no topic signal ─────────────────────────────────\n",
        "# model_path = pipe.train(mode='baseline', dataset='squad')\n",
        "# print(f\"Model saved to: {model_path}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "code-train-topic2x",
      "metadata": {
        "id": "code-train-topic2x"
      },
      "source": [
        "# ── TopicQG2X — trained on MixSQuAD2X (20k, reversed context order) ─────────\n",
        "# model_path = pipe.train(mode='topic2x', dataset='squad')\n",
        "# print(f\"Model saved to: {model_path}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "md-generate-test",
      "metadata": {
        "id": "md-generate-test"
      },
      "source": [
        "## 5. Quick Generation Test"
      ]
    },
    {
      "cell_type": "code",
      "id": "code-generate-test",
      "metadata": {
        "id": "code-generate-test"
      },
      "source": [
        "topic   = \"Electronegativity\"\n",
        "context = (\n",
        "    \"Electronegativity is a measure of the tendency of an atom to attract \"\n",
        "    \"a bonding pair of electrons. The Pauling scale is the most commonly \"\n",
        "    \"used. Fluorine has the highest electronegativity (4.0). \"\n",
        "    \"Electronegativity increases across a period and decreases down a group.\"\n",
        ")\n",
        "\n",
        "question = pipe.generate(topic=topic, context=context, mode='topic')\n",
        "print(f\"Topic   : {topic}\")\n",
        "print(f\"Question: {question}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "md-evaluate",
      "metadata": {
        "id": "md-evaluate"
      },
      "source": [
        "## 6. Evaluate\n",
        "\n",
        "Runs the full metric suite (word-level BLEU, char-level BLEU, F1, METEOR, ROUGE-L, Perplexity) and prints a comparison table against paper baselines.\n",
        "\n",
        "**KhanQ evaluation** uses the `mixkhanq/data.csv` set (653 pairs, `topic2`/`question2` columns — paper's method)."
      ]
    },
    {
      "cell_type": "code",
      "id": "code-eval",
      "metadata": {
        "id": "code-eval"
      },
      "source": [
        "# Evaluate T5 models only (no Ollama/Gemini needed)\n",
        "results = pipe.evaluate(\n",
        "    models='t5:topic',          # or 't5:baseline,t5:topic,t5:topic2x' or 'all'\n",
        "    dataset='khanq',\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "code-results",
      "metadata": {
        "id": "code-results"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "rows = []\n",
        "for key, m in results.items():\n",
        "    rows.append({\n",
        "        'model':       key,\n",
        "        'n':           m.get('num_samples', '-'),\n",
        "        'B1 (word)':   round(m.get('bleu1',      0), 3),\n",
        "        'B4 (word)':   round(m.get('bleu4',      0), 3),\n",
        "        'B1c (paper)': round(m.get('bleu1_char', 0), 3),\n",
        "        'B4c (paper)': round(m.get('bleu4_char', 0), 3),\n",
        "        'F1':          round(m.get('f1',          0), 3),\n",
        "        'METEOR':      round(m.get('meteor',      0), 3),\n",
        "        'ROUGE-L':     round(m.get('rouge_l',     0), 3),\n",
        "        'PPL':         round(m.get('perplexity',  float('nan')), 3),\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(rows).set_index('model')\n",
        "pd.set_option('display.max_columns', None)\n",
        "df"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "md-paper-baselines",
      "metadata": {
        "id": "md-paper-baselines"
      },
      "source": [
        "### Paper Baselines (char-level BLEU, KhanQ)\n",
        "\n",
        "| Model | B1c | B2c | B3c | B4c | F1 | METEOR | ROUGE-L | PPL |\n",
        "|-------|-----|-----|-----|-----|----|--------|---------|-----|\n",
        "| Baseline | 0.519 | 0.316 | 0.216 | 0.175 | 0.319 | 0.216 | 0.207 | 1.303 |\n",
        "| TopicQGedu | 0.551 | 0.335 | 0.221 | 0.177 | 0.302 | 0.216 | 0.204 | 1.360 |\n",
        "| **TopicQG** | **0.551** | **0.343** | **0.236** | **0.191** | **0.330** | **0.233** | **0.230** | **1.323** |\n",
        "| TopicQG 8-bit | 0.546 | 0.339 | 0.231 | 0.186 | 0.319 | 0.226 | 0.225 | 1.327 |\n",
        "| TopicQG 4-bit | 0.543 | 0.337 | 0.231 | 0.186 | 0.318 | 0.223 | 0.223 | 1.334 |\n",
        "| TopicQG2X | 0.536 | 0.328 | 0.221 | 0.177 | 0.321 | 0.220 | 0.216 | 1.345 |\n",
        "\n",
        "> Use `B1c`/`B4c` columns from the results table above for direct comparison."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "md-save",
      "metadata": {
        "id": "md-save"
      },
      "source": [
        "## 7. Save to Drive"
      ]
    },
    {
      "cell_type": "code",
      "id": "code-save-drive",
      "metadata": {
        "id": "code-save-drive"
      },
      "source": [
        "import shutil\n",
        "\n",
        "# Sync models and results to Drive\n",
        "for subdir in ('models', 'results'):\n",
        "    src = Path('/content/ai4ed-qg') / subdir\n",
        "    dst = DRIVE_DIR / subdir\n",
        "    if src.exists():\n",
        "        shutil.copytree(src, dst, dirs_exist_ok=True)\n",
        "        print(f\"Synced {subdir}/ to Drive\")\n",
        "\n",
        "print(f\"\\nAll files saved to: {DRIVE_DIR}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "code-download",
      "metadata": {
        "id": "code-download"
      },
      "source": [
        "# Download best model as zip\n",
        "import shutil\n",
        "from google.colab import files as colab_files\n",
        "\n",
        "model_dir = Path('/content/ai4ed-qg/models/topic/best_model')\n",
        "if model_dir.exists():\n",
        "    shutil.make_archive('/content/t5_topic_best_model', 'zip', model_dir)\n",
        "    colab_files.download('/content/t5_topic_best_model.zip')\n",
        "else:\n",
        "    print(\"Model not found — train first\")"
      ],
      "outputs": [],
      "execution_count": null
    }
  ]
}