{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "md-title",
   "metadata": {
    "id": "md-title"
   },
   "source": "# T5 Question Generation — Colab Training\n\nStandalone notebook for training and evaluating T5 topic-controlled question generation on Google Colab.\n\n**Use this notebook when:**\n- You want to train on Colab's GPU using datasets already in the repository\n- You want to evaluate with the full metric suite against paper baselines\n\n**Steps:**\n1. Setup environment and clone repo (data files included)\n2. Verify training data from the cloned repository\n3. Train one or more model variants with `pipe.train()`\n4. Evaluate with `pipe.evaluate()` against paper baselines\n5. Download the trained model\n\n**Expected CSV files** (committed to `data/training/` in the repo):\n```\ndata/training/squad/baseline/   train.csv  val.csv  test.csv\ndata/training/squad/mixsquad/   train.csv  val.csv  test.csv\ndata/training/khanq/mixkhanq/   data.csv\n```"
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "VXAIerRedBtn"
   },
   "id": "VXAIerRedBtn",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "md-setup",
   "metadata": {
    "id": "md-setup"
   },
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "id": "code-gpu",
   "metadata": {
    "id": "code-gpu",
    "outputId": "b6b5bc71-0cce-42f3-eb1b-80cf4c159127",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "# Check GPU\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"\\nPyTorch : {torch.__version__}\")\n",
    "print(f\"CUDA    : {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    name = torch.cuda.get_device_name(0)\n",
    "    mem  = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU     : {name} ({mem:.0f} GB)\")\n",
    "    # Suggest batch size based on available VRAM\n",
    "    suggested_batch = 128 if mem >= 35 else 64 if mem >= 15 else 32\n",
    "    print(f\"Suggested batch size: {suggested_batch}\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected. Training will be very slow.\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mon Feb 23 17:02:53 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.82.07              Driver Version: 580.82.07      CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   38C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "\n",
      "PyTorch : 2.10.0+cu128\n",
      "CUDA    : True\n",
      "GPU     : Tesla T4 (16 GB)\n",
      "Suggested batch size: 64\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "code-env",
   "metadata": {
    "id": "code-env",
    "outputId": "25d1853b-7a69-4f77-a82b-c85357403b53",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "import sys, os\n",
    "from pathlib import Path\n",
    "\n",
    "# ── Clone repository ──────────────────────────────────────────────────────────\n",
    "# TODO: replace with your actual repository URL\n",
    "REPO_URL = \"https://github.com/Byambaa0325/question-generation-distillation.git\"\n",
    "!git clone {REPO_URL} /content/ai4ed-qg -q\n",
    "%cd /content/ai4ed-qg\n",
    "\n",
    "# ── Install dependencies ──────────────────────────────────────────────────────\n",
    "!pip install -q torch transformers datasets accelerate sentencepiece \\\n",
    "                evaluate rouge_score nltk sentence-transformers \\\n",
    "                pyyaml tqdm pandas python-dotenv\n",
    "\n",
    "import nltk\n",
    "for res in ('punkt', 'punkt_tab', 'wordnet', 'omw-1.4'):\n",
    "    nltk.download(res, quiet=True)\n",
    "\n",
    "sys.path.insert(0, '/content/ai4ed-qg')\n",
    "os.chdir('/content/ai4ed-qg')\n",
    "print(f\"Working dir: {os.getcwd()}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content/ai4ed-qg\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Working dir: /content/ai4ed-qg\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "code-drive",
   "metadata": {
    "id": "code-drive",
    "outputId": "59a47f8c-6a67-4ef6-f562-c5f290a0b40b",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    }
   },
   "source": "# Data is included in the cloned repository — no Google Drive needed.\nfrom pathlib import Path\n\nREPO_DIR = Path('/content/ai4ed-qg')\nprint(f\"Repo    : {REPO_DIR}\")\nprint(f\"Data dir: {REPO_DIR / 'data'} — exists: {(REPO_DIR / 'data').exists()}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "md-data",
   "metadata": {
    "id": "md-data"
   },
   "source": "## 2. Verify Training Data\n\nTraining data is committed to the repository and was cloned in Step 1. No upload or Drive mounting needed.\n\nRun the cell below to confirm all expected files are present."
  },
  {
   "cell_type": "code",
   "id": "code-upload",
   "metadata": {
    "id": "code-upload"
   },
   "source": "# Verify training data from cloned repo\nfrom pathlib import Path\n\nREPO_DIR = Path('/content/ai4ed-qg')\n\ncheck_paths = [\n    'data/training/squad/baseline/train.csv',\n    'data/training/squad/baseline/val.csv',\n    'data/training/squad/baseline/test.csv',\n    'data/training/squad/mixsquad/train.csv',\n    'data/training/squad/mixsquad/val.csv',\n    'data/training/squad/mixsquad/test.csv',\n    'data/training/khanq/mixkhanq/data.csv',\n]\n\nall_ok = True\nfor rel in check_paths:\n    p = REPO_DIR / rel\n    if p.exists():\n        print(f\"  [OK]      {rel}  ({p.stat().st_size:,} bytes)\")\n    else:\n        print(f\"  [MISSING] {rel}\")\n        all_ok = False\n\nif all_ok:\n    print(\"\\nAll training files present — ready to train.\")\nelse:\n    print(\"\\nSome files missing. Available CSVs in data/training/:\")\n    for f in sorted((REPO_DIR / 'data/training').rglob('*.csv')):\n        print(f\"  {f.relative_to(REPO_DIR)}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "code-place-files",
   "metadata": {
    "id": "code-place-files",
    "outputId": "2da89ef8-5497-4f42-ad17-b41bb4f1a0f8",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": "# No file placement needed — data is already in the correct paths from the cloned repo.\nprint(\"Data paths are set up by the repository structure. Proceed to Step 3.\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "code-verify-data",
   "metadata": {
    "id": "code-verify-data"
   },
   "source": "# Preview first few rows of a training file to confirm format\nimport pandas as pd\nfrom pathlib import Path\n\nREPO_DIR = Path('/content/ai4ed-qg')\nsample_csv = REPO_DIR / 'data/training/squad/mixsquad/train.csv'\n\nif sample_csv.exists():\n    df = pd.read_csv(sample_csv)\n    print(f\"mixsquad/train.csv — {len(df):,} rows, columns: {list(df.columns)}\")\n    display(df.head(3))\nelse:\n    print(f\"File not found: {sample_csv}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "md-init",
   "metadata": {
    "id": "md-init"
   },
   "source": [
    "## 3. Initialise Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "id": "code-init",
   "metadata": {
    "id": "code-init",
    "outputId": "28254e7e-3fa5-4cad-9551-1de1b305aea6",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "from src.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline('config/pipeline.yaml')\n",
    "pipe.status()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Pipeline status:\n",
      "  [-] convert.squad.text\n",
      "  [-] convert.squad.question\n",
      "  [-] convert.khanq.text\n",
      "  [-] convert.khanq.question\n",
      "  [-] wikify.squad.text\n",
      "  [-] wikify.squad.question\n",
      "  [-] wikify.khanq.text\n",
      "  [-] wikify.khanq.question\n",
      "  [-] topics.squad.enriched\n",
      "  [-] topics.squad.filtered\n",
      "  [-] topics.khanq.enriched\n",
      "  [-] topics.khanq.filtered\n",
      "  [-] dataset.squad.baseline\n",
      "  [-] dataset.squad.mixsquad\n",
      "  [-] dataset.squad.mixsquad2x\n",
      "  [-] dataset.khanq.baseline\n",
      "  [-] dataset.khanq.mixsquad\n",
      "  [-] dataset.khanq.mixsquad2x\n",
      "  [-] train.baseline\n",
      "  [-] train.topic\n",
      "  [-] train.topic2x\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'convert.squad.text': False,\n",
       " 'convert.squad.question': False,\n",
       " 'convert.khanq.text': False,\n",
       " 'convert.khanq.question': False,\n",
       " 'wikify.squad.text': False,\n",
       " 'wikify.squad.question': False,\n",
       " 'wikify.khanq.text': False,\n",
       " 'wikify.khanq.question': False,\n",
       " 'topics.squad.enriched': False,\n",
       " 'topics.squad.filtered': False,\n",
       " 'topics.khanq.enriched': False,\n",
       " 'topics.khanq.filtered': False,\n",
       " 'dataset.squad.baseline': False,\n",
       " 'dataset.squad.mixsquad': False,\n",
       " 'dataset.squad.mixsquad2x': False,\n",
       " 'dataset.khanq.baseline': False,\n",
       " 'dataset.khanq.mixsquad': False,\n",
       " 'dataset.khanq.mixsquad2x': False,\n",
       " 'train.baseline': False,\n",
       " 'train.topic': False,\n",
       " 'train.topic2x': False}"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "code-config",
   "metadata": {
    "id": "code-config",
    "outputId": "f601f2bf-cdc8-4dcb-cc6c-0b6f669a3b94",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "# ── Tweak training config for your GPU ───────────────────────────────────────\n",
    "# Edit pipeline.yaml to make changes permanent, or override here:\n",
    "\n",
    "pipe.config.training.batch  = 64     # 128 for A100, 64 for T4/V100, 32 if OOM\n",
    "pipe.config.training.epochs = 50     # paper uses 50\n",
    "pipe.config.training.lr     = 1e-3\n",
    "\n",
    "t = pipe.config.training\n",
    "print(f\"Model      : {t.model_name}\")\n",
    "print(f\"Batch size : {t.batch}\")\n",
    "print(f\"Epochs     : {t.epochs}\")\n",
    "print(f\"LR         : {t.lr}\")\n",
    "print(f\"Max input  : {t.max_input_len} tokens\")\n",
    "print(f\"Max output : {t.max_output_len} tokens\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model      : google-t5/t5-small\n",
      "Batch size : 64\n",
      "Epochs     : 50\n",
      "LR         : 0.001\n",
      "Max input  : 200 tokens\n",
      "Max output : 45 tokens\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "md-train",
   "metadata": {
    "id": "md-train"
   },
   "source": [
    "## 4. Train\n",
    "\n",
    "Train the model variant you need. The pipeline uses the correct paper format for all modes:\n",
    "```\n",
    "Input:  <topic> {topic} <context> {combined text}\n",
    "Target: {question}\n",
    "```\n",
    "\n",
    "Saved to `models/{mode}/best_model/` (best checkpoint by validation loss)."
   ]
  },
  {
   "cell_type": "code",
   "id": "code-train-topic",
   "metadata": {
    "id": "code-train-topic"
   },
   "source": [
    "# ── TopicQG — trained on MixSQuAD (10k mixed pairs) ─────────────────────────\n",
    "model_path = pipe.train(mode='topic', dataset='squad')\n",
    "print(f\"\\nModel saved to: {model_path}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "code-train-baseline",
   "metadata": {
    "id": "code-train-baseline"
   },
   "source": [
    "# ── Baseline — context only, no topic signal ─────────────────────────────────\n",
    "# model_path = pipe.train(mode='baseline', dataset='squad')\n",
    "# print(f\"Model saved to: {model_path}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "code-train-topic2x",
   "metadata": {
    "id": "code-train-topic2x"
   },
   "source": [
    "# ── TopicQG2X — trained on MixSQuAD2X (20k, reversed context order) ─────────\n",
    "# model_path = pipe.train(mode='topic2x', dataset='squad')\n",
    "# print(f\"Model saved to: {model_path}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "md-generate-test",
   "metadata": {
    "id": "md-generate-test"
   },
   "source": [
    "## 5. Quick Generation Test"
   ]
  },
  {
   "cell_type": "code",
   "id": "code-generate-test",
   "metadata": {
    "id": "code-generate-test"
   },
   "source": [
    "topic   = \"Electronegativity\"\n",
    "context = (\n",
    "    \"Electronegativity is a measure of the tendency of an atom to attract \"\n",
    "    \"a bonding pair of electrons. The Pauling scale is the most commonly \"\n",
    "    \"used. Fluorine has the highest electronegativity (4.0). \"\n",
    "    \"Electronegativity increases across a period and decreases down a group.\"\n",
    ")\n",
    "\n",
    "question = pipe.generate(topic=topic, context=context, mode='topic')\n",
    "print(f\"Topic   : {topic}\")\n",
    "print(f\"Question: {question}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "md-evaluate",
   "metadata": {
    "id": "md-evaluate"
   },
   "source": [
    "## 6. Evaluate\n",
    "\n",
    "Runs the full metric suite (word-level BLEU, char-level BLEU, F1, METEOR, ROUGE-L, Perplexity) and prints a comparison table against paper baselines.\n",
    "\n",
    "**KhanQ evaluation** uses the `mixkhanq/data.csv` set (653 pairs, `topic2`/`question2` columns — paper's method)."
   ]
  },
  {
   "cell_type": "code",
   "id": "code-eval",
   "metadata": {
    "id": "code-eval"
   },
   "source": [
    "# Evaluate T5 models only (no Ollama/Gemini needed)\n",
    "results = pipe.evaluate(\n",
    "    models='t5:topic',          # or 't5:baseline,t5:topic,t5:topic2x' or 'all'\n",
    "    dataset='khanq',\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "code-results",
   "metadata": {
    "id": "code-results"
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "rows = []\n",
    "for key, m in results.items():\n",
    "    rows.append({\n",
    "        'model':       key,\n",
    "        'n':           m.get('num_samples', '-'),\n",
    "        'B1 (word)':   round(m.get('bleu1',      0), 3),\n",
    "        'B4 (word)':   round(m.get('bleu4',      0), 3),\n",
    "        'B1c (paper)': round(m.get('bleu1_char', 0), 3),\n",
    "        'B4c (paper)': round(m.get('bleu4_char', 0), 3),\n",
    "        'F1':          round(m.get('f1',          0), 3),\n",
    "        'METEOR':      round(m.get('meteor',      0), 3),\n",
    "        'ROUGE-L':     round(m.get('rouge_l',     0), 3),\n",
    "        'PPL':         round(m.get('perplexity',  float('nan')), 3),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows).set_index('model')\n",
    "pd.set_option('display.max_columns', None)\n",
    "df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "md-paper-baselines",
   "metadata": {
    "id": "md-paper-baselines"
   },
   "source": [
    "### Paper Baselines (char-level BLEU, KhanQ)\n",
    "\n",
    "| Model | B1c | B2c | B3c | B4c | F1 | METEOR | ROUGE-L | PPL |\n",
    "|-------|-----|-----|-----|-----|----|--------|---------|-----|\n",
    "| Baseline | 0.519 | 0.316 | 0.216 | 0.175 | 0.319 | 0.216 | 0.207 | 1.303 |\n",
    "| TopicQGedu | 0.551 | 0.335 | 0.221 | 0.177 | 0.302 | 0.216 | 0.204 | 1.360 |\n",
    "| **TopicQG** | **0.551** | **0.343** | **0.236** | **0.191** | **0.330** | **0.233** | **0.230** | **1.323** |\n",
    "| TopicQG 8-bit | 0.546 | 0.339 | 0.231 | 0.186 | 0.319 | 0.226 | 0.225 | 1.327 |\n",
    "| TopicQG 4-bit | 0.543 | 0.337 | 0.231 | 0.186 | 0.318 | 0.223 | 0.223 | 1.334 |\n",
    "| TopicQG2X | 0.536 | 0.328 | 0.221 | 0.177 | 0.321 | 0.220 | 0.216 | 1.345 |\n",
    "\n",
    "> Use `B1c`/`B4c` columns from the results table above for direct comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-save",
   "metadata": {
    "id": "md-save"
   },
   "source": "## 7. Download Trained Model\n\nDownload the trained model as a zip file to your local machine. The cell below zips `models/topic/best_model/` and triggers a browser download."
  },
  {
   "cell_type": "code",
   "id": "code-save-drive",
   "metadata": {
    "id": "code-save-drive"
   },
   "source": "# Optional: save results summary to a local file before downloading\nimport json\nfrom pathlib import Path\n\nresults_dir = Path('/content/ai4ed-qg/results')\nresults_dir.mkdir(parents=True, exist_ok=True)\n\nif 'results' in dir():\n    out = results_dir / 'eval_results.json'\n    with open(out, 'w') as f:\n        json.dump(results, f, indent=2, default=str)\n    print(f\"Results saved to: {out}\")\nelse:\n    print(\"No evaluation results yet — run Section 6 first.\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "code-download",
   "metadata": {
    "id": "code-download"
   },
   "source": [
    "# Download best model as zip\n",
    "import shutil\n",
    "from google.colab import files as colab_files\n",
    "\n",
    "model_dir = Path('/content/ai4ed-qg/models/topic/best_model')\n",
    "if model_dir.exists():\n",
    "    shutil.make_archive('/content/t5_topic_best_model', 'zip', model_dir)\n",
    "    colab_files.download('/content/t5_topic_best_model.zip')\n",
    "else:\n",
    "    print(\"Model not found — train first\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ]
}