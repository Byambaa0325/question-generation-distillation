{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "md-title",
   "metadata": {
    "id": "md-title"
   },
   "source": "# T5 Question Generation — Colab Training\n\nStandalone notebook for training and evaluating T5 topic-controlled question generation on Google Colab.\n\n**Use this notebook when:**\n- You want to train on Colab's GPU using datasets already in the repository\n- You want to evaluate with the full metric suite against paper baselines\n\n**Steps:**\n1. Setup environment and clone repo (data files included)\n2. Verify training data from the cloned repository\n3. Train one or more model variants with `pipe.train()`\n4. Evaluate with `pipe.evaluate()` against paper baselines\n5. Download the trained model\n\n**Expected CSV files** (committed to `data/training/` in the repo):\n```\ndata/training/squad/baseline/   train.csv  val.csv  test.csv\ndata/training/squad/mixsquad/   train.csv  val.csv  test.csv\ndata/training/khanq/mixkhanq/   data.csv\n```"
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "VXAIerRedBtn"
   },
   "id": "VXAIerRedBtn",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "md-setup",
   "metadata": {
    "id": "md-setup"
   },
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "id": "code-gpu",
   "metadata": {
    "id": "code-gpu",
    "outputId": "b6b5bc71-0cce-42f3-eb1b-80cf4c159127",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "# Check GPU\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"\\nPyTorch : {torch.__version__}\")\n",
    "print(f\"CUDA    : {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    name = torch.cuda.get_device_name(0)\n",
    "    mem  = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU     : {name} ({mem:.0f} GB)\")\n",
    "    # Suggest batch size based on available VRAM\n",
    "    suggested_batch = 128 if mem >= 35 else 64 if mem >= 15 else 32\n",
    "    print(f\"Suggested batch size: {suggested_batch}\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected. Training will be very slow.\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mon Feb 23 17:02:53 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.82.07              Driver Version: 580.82.07      CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   38C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "\n",
      "PyTorch : 2.10.0+cu128\n",
      "CUDA    : True\n",
      "GPU     : Tesla T4 (16 GB)\n",
      "Suggested batch size: 64\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "code-env",
   "metadata": {
    "id": "code-env",
    "outputId": "25d1853b-7a69-4f77-a82b-c85357403b53",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": "import sys, os\nfrom pathlib import Path\n\n# ── Clone repository ──────────────────────────────────────────────────────────\n# TODO: replace with your actual repository URL\nREPO_URL = \"https://github.com/Byambaa0325/question-generation-distillation.git\"\n!git clone {REPO_URL} /content/ai4ed-qg -q\n%cd /content/ai4ed-qg\n\n# ── Install dependencies ──────────────────────────────────────────────────────\n# transformers>=4.46 required: eval_strategy + processing_class API (replaces\n# evaluation_strategy + tokenizer= which were removed in 4.46)\n!pip install -q torch \"transformers>=4.46.0\" datasets accelerate sentencepiece \\\n                evaluate rouge_score nltk sentence-transformers \\\n                pyyaml tqdm pandas python-dotenv\n\nimport nltk\nfor res in ('punkt', 'punkt_tab', 'wordnet', 'omw-1.4'):\n    nltk.download(res, quiet=True)\n\nsys.path.insert(0, '/content/ai4ed-qg')\nos.chdir('/content/ai4ed-qg')\nprint(f\"Working dir: {os.getcwd()}\")\n\nimport transformers\nprint(f\"transformers: {transformers.__version__}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "code-drive",
   "metadata": {
    "id": "code-drive",
    "outputId": "59a47f8c-6a67-4ef6-f562-c5f290a0b40b",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    }
   },
   "source": "# Data is included in the cloned repository — no Google Drive needed.\nfrom pathlib import Path\n\nREPO_DIR = Path('/content/ai4ed-qg')\nprint(f\"Repo    : {REPO_DIR}\")\nprint(f\"Data dir: {REPO_DIR / 'data'} — exists: {(REPO_DIR / 'data').exists()}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "md-data",
   "metadata": {
    "id": "md-data"
   },
   "source": "## 2. Verify Training Data\n\nTraining data is committed to the repository and was cloned in Step 1. No upload or Drive mounting needed.\n\nRun the cell below to confirm all expected files are present."
  },
  {
   "cell_type": "code",
   "id": "code-upload",
   "metadata": {
    "id": "code-upload"
   },
   "source": "# Verify training data from cloned repo\nfrom pathlib import Path\n\nREPO_DIR = Path('/content/ai4ed-qg')\n\ncheck_paths = [\n    # SQuAD — baseline (WAT, plain context, 70/15/15 split of 37,388 entries)\n    'data/training/squad/baseline/train.csv',\n    'data/training/squad/baseline/val.csv',\n    'data/training/squad/baseline/test.csv',\n    # SQuAD — MixSQuAD (WAT, 10k random mixed-context pairs)\n    'data/training/squad/mixsquad/train.csv',\n    'data/training/squad/mixsquad/val.csv',\n    'data/training/squad/mixsquad/test.csv',\n    # SQuAD — MixSQuAD2X (MixSQuAD doubled, 20k entries)\n    'data/training/squad/mixsquad2x/train.csv',\n    'data/training/squad/mixsquad2x/val.csv',\n    'data/training/squad/mixsquad2x/test.csv',\n    # KhanQ — MixKhanQ evaluation set (Wikifier, 653 entries, no split)\n    'data/training/khanq/mixkhanq/data.csv',\n]\n\nall_ok = True\nprev_group = None\nfor rel in check_paths:\n    group = '/'.join(rel.split('/')[:4])\n    if group != prev_group:\n        print()\n        prev_group = group\n    p = REPO_DIR / rel\n    if p.exists():\n        print(f\"  [OK]      {rel}  ({p.stat().st_size:,} bytes)\")\n    else:\n        print(f\"  [MISSING] {rel}\")\n        all_ok = False\n\nprint()\nif all_ok:\n    print(\"All training files present — ready to train.\")\nelse:\n    print(\"Some files missing. Available CSVs in data/training/:\")\n    for f in sorted((REPO_DIR / 'data/training').rglob('*.csv')):\n        print(f\"  {f.relative_to(REPO_DIR)}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "code-place-files",
   "metadata": {
    "id": "code-place-files",
    "outputId": "2da89ef8-5497-4f42-ad17-b41bb4f1a0f8",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": "# No file placement needed — data is already in the correct paths from the cloned repo.\nprint(\"Data paths are set up by the repository structure. Proceed to Step 3.\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "code-verify-data",
   "metadata": {
    "id": "code-verify-data"
   },
   "source": "# Preview first few rows of a training file to confirm format\nimport pandas as pd\nfrom pathlib import Path\n\nREPO_DIR = Path('/content/ai4ed-qg')\nsample_csv = REPO_DIR / 'data/training/squad/mixsquad/train.csv'\n\nif sample_csv.exists():\n    df = pd.read_csv(sample_csv)\n    print(f\"mixsquad/train.csv — {len(df):,} rows, columns: {list(df.columns)}\")\n    display(df.head(3))\nelse:\n    print(f\"File not found: {sample_csv}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "md-init",
   "metadata": {
    "id": "md-init"
   },
   "source": [
    "## 3. Initialise Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "id": "code-init",
   "metadata": {
    "id": "code-init",
    "outputId": "28254e7e-3fa5-4cad-9551-1de1b305aea6",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "from src.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline('config/pipeline.yaml')\n",
    "pipe.status()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Pipeline status:\n",
      "  [-] convert.squad.text\n",
      "  [-] convert.squad.question\n",
      "  [-] convert.khanq.text\n",
      "  [-] convert.khanq.question\n",
      "  [-] wikify.squad.text\n",
      "  [-] wikify.squad.question\n",
      "  [-] wikify.khanq.text\n",
      "  [-] wikify.khanq.question\n",
      "  [-] topics.squad.enriched\n",
      "  [-] topics.squad.filtered\n",
      "  [-] topics.khanq.enriched\n",
      "  [-] topics.khanq.filtered\n",
      "  [-] dataset.squad.baseline\n",
      "  [-] dataset.squad.mixsquad\n",
      "  [-] dataset.squad.mixsquad2x\n",
      "  [-] dataset.khanq.baseline\n",
      "  [-] dataset.khanq.mixsquad\n",
      "  [-] dataset.khanq.mixsquad2x\n",
      "  [-] train.baseline\n",
      "  [-] train.topic\n",
      "  [-] train.topic2x\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'convert.squad.text': False,\n",
       " 'convert.squad.question': False,\n",
       " 'convert.khanq.text': False,\n",
       " 'convert.khanq.question': False,\n",
       " 'wikify.squad.text': False,\n",
       " 'wikify.squad.question': False,\n",
       " 'wikify.khanq.text': False,\n",
       " 'wikify.khanq.question': False,\n",
       " 'topics.squad.enriched': False,\n",
       " 'topics.squad.filtered': False,\n",
       " 'topics.khanq.enriched': False,\n",
       " 'topics.khanq.filtered': False,\n",
       " 'dataset.squad.baseline': False,\n",
       " 'dataset.squad.mixsquad': False,\n",
       " 'dataset.squad.mixsquad2x': False,\n",
       " 'dataset.khanq.baseline': False,\n",
       " 'dataset.khanq.mixsquad': False,\n",
       " 'dataset.khanq.mixsquad2x': False,\n",
       " 'train.baseline': False,\n",
       " 'train.topic': False,\n",
       " 'train.topic2x': False}"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "code-config",
   "metadata": {
    "id": "code-config",
    "outputId": "f601f2bf-cdc8-4dcb-cc6c-0b6f669a3b94",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": "import torch\nfrom src.pipeline import Pipeline\n\npipe = Pipeline('config/pipeline.yaml')\ntc = pipe.config.training\n\n# ── Auto-tune to the available GPU ───────────────────────────────────────────\nif torch.cuda.is_available():\n    name = torch.cuda.get_device_name(0)\n    vram = torch.cuda.get_device_properties(0).total_memory / 1e9\n\n    # bf16 is natively fast on Ampere (A100) and Hopper (H100).\n    # fp16 is the right choice for Turing (T4) and Volta (V100).\n    # fp32 fallback for everything else.\n    supports_bf16 = torch.cuda.is_bf16_supported()\n\n    if vram >= 70:           # H100 80 GB\n        tc.batch, tc.fp16, tc.bf16, tc.grad_accum = 256, False, True,  1\n    elif vram >= 38:         # A100 40/80 GB\n        tc.batch, tc.fp16, tc.bf16, tc.grad_accum = 128, False, True,  1\n    elif vram >= 14:         # T4 16 GB  /  V100 16 GB\n        tc.batch, tc.fp16, tc.bf16, tc.grad_accum = 64,  not supports_bf16, supports_bf16, 1\n    else:                    # smaller GPU / CPU — use grad accumulation to compensate\n        tc.batch, tc.fp16, tc.bf16, tc.grad_accum = 16,  False, False, 4\n\n    tc.dataloader_workers = 2   # Colab is Linux — safe to use background workers\nelse:\n    name, vram = \"CPU\", 0\n\n# ── Anti-overfitting defaults ─────────────────────────────────────────────────\n# Val loss bottomed at ~1.77 around epoch 5, then rose to ~3.0 by epoch 50.\n# Early stopping + weight decay fix this without tuning epochs by hand.\ntc.epochs                    = 25     # safety cap; early stopping fires first\ntc.lr                        = 1e-3\ntc.warmup_steps              = 200    # ~2 epochs on T4 (7k samples, batch 64)\ntc.weight_decay              = 0.01   # L2 regularisation\ntc.early_stopping_patience   = 5      # stop after 5 epochs with no val improvement\n\n# ── Manual overrides (uncomment to adjust) ───────────────────────────────────\n# tc.batch                   = 32     # reduce if CUDA OOM\n# tc.grad_accum              = 2      # effective batch = batch * grad_accum\n# tc.lr                      = 5e-4   # lower LR if val curve is still noisy\n# tc.early_stopping_patience = 0      # 0 = disabled (run all epochs)\n\nprint(f\"GPU          : {name} ({vram:.0f} GB)\")\nprint(f\"Batch        : {tc.batch}  (effective: {tc.batch * tc.grad_accum}  grad_accum={tc.grad_accum})\")\nprint(f\"Precision    : {'bf16' if tc.bf16 else 'fp16' if tc.fp16 else 'fp32'}\")\nprint(f\"Epochs (max) : {tc.epochs}   LR: {tc.lr}   warmup_steps: {tc.warmup_steps}\")\nprint(f\"Weight decay : {tc.weight_decay}   early_stopping_patience: {tc.early_stopping_patience}\")\nprint(f\"Workers      : {tc.dataloader_workers}\")\nprint(f\"Model        : {tc.model_name}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8u9vp3axhwu",
   "source": "## 4. Hyperparameter Sweep (optional)\n\nGrid search over `lr` × `weight_decay` to find the best combination before committing to a full 25-epoch run. Each trial runs up to 15 epochs with early stopping patience=3, so most trials exit in 6–9 epochs.\n\n**Grid (9 runs):**\n\n| | `wd=0.0` | `wd=0.01` | `wd=0.1` |\n|---|---|---|---|\n| `lr=1e-3` | run 1 | run 2 | run 3 |\n| `lr=5e-4` | run 4 | run 5 | run 6 |\n| `lr=3e-4` | run 7 | run 8 | run 9 |\n\n**Estimated time:** ~3–4 min/run on T4 → ~30 min total. ~8 min total on A100.\n\nSkip to **Section 5** if you want to train directly with the defaults from Section 3.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "0ptzem7xspw",
   "source": "import shutil, itertools\nimport torch\nimport pandas as pd\nfrom pathlib import Path\nfrom torch.utils.data import Dataset\nfrom transformers import (\n    DataCollatorForSeq2Seq, EarlyStoppingCallback,\n    Seq2SeqTrainer, Seq2SeqTrainingArguments,\n    T5ForConditionalGeneration, T5Tokenizer,\n)\n\n# ── Inline dataset class (mirrors train.py) ───────────────────────────────────\nclass _QGDataset(Dataset):\n    def __init__(self, data_file, tokenizer, max_input_len, max_output_len):\n        df = pd.read_csv(data_file)\n        self.tokenizer = tokenizer\n        self.max_input_len  = max_input_len\n        self.max_output_len = max_output_len\n        self.examples = [\n            {\n                \"input_text\":  f\"<topic> {row['topic']} <context> {row['text']} \",\n                \"target_text\": str(row[\"question\"]),\n            }\n            for _, row in df.iterrows()\n        ]\n\n    def __len__(self):  return len(self.examples)\n\n    def __getitem__(self, idx):\n        ex = self.examples[idx]\n        enc = self.tokenizer(\n            ex[\"input_text\"], max_length=self.max_input_len,\n            padding=\"max_length\", truncation=True,\n        )\n        labels = self.tokenizer(\n            ex[\"target_text\"], max_length=self.max_output_len,\n            padding=\"max_length\", truncation=True,\n        ).input_ids\n        labels = [(l if l != self.tokenizer.pad_token_id else -100) for l in labels]\n        return {\"input_ids\": enc[\"input_ids\"], \"attention_mask\": enc[\"attention_mask\"], \"labels\": labels}\n\n\ndef sweep_run(train_csv, val_csv, tc, hparams, run_idx):\n    \"\"\"\n    Train one sweep trial. Returns metrics dict.\n    Output is written to /tmp/sweep_{run_idx}/ and deleted afterwards.\n    \"\"\"\n    run_dir = Path(f\"/tmp/sweep_{run_idx}\")\n    run_dir.mkdir(parents=True, exist_ok=True)\n\n    tokenizer = T5Tokenizer.from_pretrained(tc.model_name, legacy=False)\n    model     = T5ForConditionalGeneration.from_pretrained(tc.model_name)\n    tokenizer.add_tokens(tc.special_tokens)\n    model.resize_token_embeddings(len(tokenizer))\n\n    train_ds = _QGDataset(train_csv, tokenizer, tc.max_input_len, tc.max_output_len)\n    val_ds   = _QGDataset(val_csv,   tokenizer, tc.max_input_len, tc.max_output_len)\n\n    args = Seq2SeqTrainingArguments(\n        output_dir           = str(run_dir),\n        num_train_epochs     = hparams.get(\"max_sweep_epochs\", 15),\n        per_device_train_batch_size = tc.batch,\n        per_device_eval_batch_size  = tc.batch,\n        learning_rate        = hparams[\"lr\"],\n        warmup_steps         = hparams.get(\"warmup_steps\", 200),\n        weight_decay         = hparams[\"weight_decay\"],\n        fp16                 = tc.fp16,\n        bf16                 = tc.bf16,\n        gradient_accumulation_steps = tc.grad_accum,\n        save_strategy        = \"epoch\",\n        eval_strategy        = \"epoch\",\n        load_best_model_at_end    = True,\n        metric_for_best_model     = \"eval_loss\",\n        greater_is_better    = False,\n        save_total_limit     = 1,\n        predict_with_generate= False,\n        logging_steps        = 99999,   # suppress per-step output\n        dataloader_num_workers = tc.dataloader_workers,\n        report_to            = \"none\",\n    )\n\n    collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=True, label_pad_token_id=-100)\n    trainer  = Seq2SeqTrainer(\n        model            = model,\n        args             = args,\n        train_dataset    = train_ds,\n        eval_dataset     = val_ds,\n        processing_class = tokenizer,\n        data_collator    = collator,\n        callbacks        = [EarlyStoppingCallback(early_stopping_patience=hparams.get(\"patience\", 3))],\n    )\n\n    trainer.train()\n\n    eval_logs = [l for l in trainer.state.log_history if \"eval_loss\" in l]\n    best      = min(eval_logs, key=lambda x: x[\"eval_loss\"]) if eval_logs else {}\n\n    shutil.rmtree(run_dir, ignore_errors=True)   # clean up temp checkpoint\n\n    return {\n        **hparams,\n        \"best_val_loss\": round(best.get(\"eval_loss\", float(\"nan\")), 5),\n        \"best_epoch\":    round(best.get(\"epoch\",     -1),           1),\n        \"epochs_run\":    round(trainer.state.epoch,                 1),\n        \"n_train\":       len(train_ds),\n    }\n\nprint(\"sweep_run() defined — ready to run grid.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "r71lhbq79mb",
   "source": "# ── Run hyperparameter sweep ──────────────────────────────────────────────────\n# Grid: lr × weight_decay (9 trials, ~30 min on T4)\n# Prerequisite: run Section 3 (code-config) first so `tc` and `pipe` are defined.\n\nfrom pathlib import Path\nimport itertools\n\nREPO_DIR  = Path('/content/ai4ed-qg')\ntrain_csv = REPO_DIR / 'data/training/squad/mixsquad/train.csv'\nval_csv   = REPO_DIR / 'data/training/squad/mixsquad/val.csv'\n\nlr_values  = [1e-3, 5e-4, 3e-4]\nwd_values  = [0.0,  0.01, 0.1]\nsweep_grid = list(itertools.product(lr_values, wd_values))\n\nsweep_results = []\nfor i, (lr, wd) in enumerate(sweep_grid):\n    hparams = {\n        \"lr\":               lr,\n        \"weight_decay\":     wd,\n        \"warmup_steps\":     200,\n        \"patience\":         3,\n        \"max_sweep_epochs\": 15,\n    }\n    print(f\"\\n[{i+1}/{len(sweep_grid)}] lr={lr}  weight_decay={wd}\")\n    result = sweep_run(train_csv, val_csv, tc, hparams, run_idx=i)\n    sweep_results.append(result)\n    print(f\"  best_val_loss={result['best_val_loss']}  \"\n          f\"best_epoch={result['best_epoch']}  \"\n          f\"epochs_run={result['epochs_run']}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "zerfuv5fy3",
   "source": "# ── Sweep results + apply best config ────────────────────────────────────────\nimport pandas as pd\n\ndf_sweep = (\n    pd.DataFrame(sweep_results)\n    [[\"lr\", \"weight_decay\", \"warmup_steps\", \"best_val_loss\", \"best_epoch\", \"epochs_run\"]]\n    .sort_values(\"best_val_loss\")\n    .reset_index(drop=True)\n)\ndf_sweep.index += 1   # 1-based rank\nprint(\"Sweep results (sorted by val loss):\")\ndisplay(df_sweep)\n\n# Apply best hyperparameters to tc for the full training run in Section 5\nbest = df_sweep.iloc[0]\ntc.lr           = float(best[\"lr\"])\ntc.weight_decay = float(best[\"weight_decay\"])\ntc.warmup_steps = int(best[\"warmup_steps\"])\ntc.epochs       = 25    # restore full epoch cap\n\nprint(f\"\\nBest config applied to tc:\")\nprint(f\"  lr={tc.lr}  weight_decay={tc.weight_decay}  warmup_steps={tc.warmup_steps}\")\nprint(f\"  -> Now run Section 5 (code-train-topic) to train with these settings.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "md-train",
   "metadata": {
    "id": "md-train"
   },
   "source": "## 4. Train\n\nTrain the model variant you need. The pipeline uses the correct paper format for all modes:\n```\nInput:  <topic> {topic} <context> {combined text}\nTarget: {question}\n```\n\nSaved to `models/{mode}/best_model/` (best checkpoint by validation loss).\n\n### Training best practices\n\n**Overfitting observed:** val loss bottomed at ~1.77 around epoch 5, then climbed to ~3.0 by epoch 50.\nWith 7k training samples and T5-small, the model memorises the data well before 50 epochs.\n\n| Practice | Why | Setting |\n|----------|-----|---------|\n| **Early stopping** | Halt once val loss stops improving — no need to guess epochs | `early_stopping_patience=5` (default) |\n| **Weight decay** | L2 regularisation penalises large weights, slows memorisation | `weight_decay=0.01` (default) |\n| **Warmup steps** | Avoids large gradient updates in the first few steps | `warmup_steps=200` (~2 epochs on T4) |\n| **Fewer max epochs** | Safety cap — early stopping should fire first, but 20–25 is safer than 50 | `epochs=25` |\n| **Lower LR** | If still overfitting, try `5e-4`; slower but smoother val curve | `lr=5e-4` |\n\n> `load_best_model_at_end=True` is always on — the saved checkpoint is the epoch with the **lowest val loss**, not the last epoch."
  },
  {
   "cell_type": "code",
   "id": "code-train-topic",
   "metadata": {
    "id": "code-train-topic"
   },
   "source": [
    "# ── TopicQG — trained on MixSQuAD (10k mixed pairs) ─────────────────────────\n",
    "model_path = pipe.train(mode='topic', dataset='squad')\n",
    "print(f\"\\nModel saved to: {model_path}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "code-train-baseline",
   "metadata": {
    "id": "code-train-baseline"
   },
   "source": [
    "# ── Baseline — context only, no topic signal ─────────────────────────────────\n",
    "# model_path = pipe.train(mode='baseline', dataset='squad')\n",
    "# print(f\"Model saved to: {model_path}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "code-train-topic2x",
   "metadata": {
    "id": "code-train-topic2x"
   },
   "source": [
    "# ── TopicQG2X — trained on MixSQuAD2X (20k, reversed context order) ─────────\n",
    "# model_path = pipe.train(mode='topic2x', dataset='squad')\n",
    "# print(f\"Model saved to: {model_path}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "md-generate-test",
   "metadata": {
    "id": "md-generate-test"
   },
   "source": [
    "## 5. Quick Generation Test"
   ]
  },
  {
   "cell_type": "code",
   "id": "code-generate-test",
   "metadata": {
    "id": "code-generate-test"
   },
   "source": [
    "topic   = \"Electronegativity\"\n",
    "context = (\n",
    "    \"Electronegativity is a measure of the tendency of an atom to attract \"\n",
    "    \"a bonding pair of electrons. The Pauling scale is the most commonly \"\n",
    "    \"used. Fluorine has the highest electronegativity (4.0). \"\n",
    "    \"Electronegativity increases across a period and decreases down a group.\"\n",
    ")\n",
    "\n",
    "question = pipe.generate(topic=topic, context=context, mode='topic')\n",
    "print(f\"Topic   : {topic}\")\n",
    "print(f\"Question: {question}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "md-evaluate",
   "metadata": {
    "id": "md-evaluate"
   },
   "source": [
    "## 6. Evaluate\n",
    "\n",
    "Runs the full metric suite (word-level BLEU, char-level BLEU, F1, METEOR, ROUGE-L, Perplexity) and prints a comparison table against paper baselines.\n",
    "\n",
    "**KhanQ evaluation** uses the `mixkhanq/data.csv` set (653 pairs, `topic2`/`question2` columns — paper's method)."
   ]
  },
  {
   "cell_type": "code",
   "id": "code-eval",
   "metadata": {
    "id": "code-eval"
   },
   "source": [
    "# Evaluate T5 models only (no Ollama/Gemini needed)\n",
    "results = pipe.evaluate(\n",
    "    models='t5:topic',          # or 't5:baseline,t5:topic,t5:topic2x' or 'all'\n",
    "    dataset='khanq',\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "code-results",
   "metadata": {
    "id": "code-results"
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "rows = []\n",
    "for key, m in results.items():\n",
    "    rows.append({\n",
    "        'model':       key,\n",
    "        'n':           m.get('num_samples', '-'),\n",
    "        'B1 (word)':   round(m.get('bleu1',      0), 3),\n",
    "        'B4 (word)':   round(m.get('bleu4',      0), 3),\n",
    "        'B1c (paper)': round(m.get('bleu1_char', 0), 3),\n",
    "        'B4c (paper)': round(m.get('bleu4_char', 0), 3),\n",
    "        'F1':          round(m.get('f1',          0), 3),\n",
    "        'METEOR':      round(m.get('meteor',      0), 3),\n",
    "        'ROUGE-L':     round(m.get('rouge_l',     0), 3),\n",
    "        'PPL':         round(m.get('perplexity',  float('nan')), 3),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows).set_index('model')\n",
    "pd.set_option('display.max_columns', None)\n",
    "df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "md-paper-baselines",
   "metadata": {
    "id": "md-paper-baselines"
   },
   "source": [
    "### Paper Baselines (char-level BLEU, KhanQ)\n",
    "\n",
    "| Model | B1c | B2c | B3c | B4c | F1 | METEOR | ROUGE-L | PPL |\n",
    "|-------|-----|-----|-----|-----|----|--------|---------|-----|\n",
    "| Baseline | 0.519 | 0.316 | 0.216 | 0.175 | 0.319 | 0.216 | 0.207 | 1.303 |\n",
    "| TopicQGedu | 0.551 | 0.335 | 0.221 | 0.177 | 0.302 | 0.216 | 0.204 | 1.360 |\n",
    "| **TopicQG** | **0.551** | **0.343** | **0.236** | **0.191** | **0.330** | **0.233** | **0.230** | **1.323** |\n",
    "| TopicQG 8-bit | 0.546 | 0.339 | 0.231 | 0.186 | 0.319 | 0.226 | 0.225 | 1.327 |\n",
    "| TopicQG 4-bit | 0.543 | 0.337 | 0.231 | 0.186 | 0.318 | 0.223 | 0.223 | 1.334 |\n",
    "| TopicQG2X | 0.536 | 0.328 | 0.221 | 0.177 | 0.321 | 0.220 | 0.216 | 1.345 |\n",
    "\n",
    "> Use `B1c`/`B4c` columns from the results table above for direct comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-save",
   "metadata": {
    "id": "md-save"
   },
   "source": "## 7. Download Trained Model\n\nDownload the trained model as a zip file to your local machine. The cell below zips `models/topic/best_model/` and triggers a browser download."
  },
  {
   "cell_type": "code",
   "id": "code-save-drive",
   "metadata": {
    "id": "code-save-drive"
   },
   "source": "# Optional: save results summary to a local file before downloading\nimport json\nfrom pathlib import Path\n\nresults_dir = Path('/content/ai4ed-qg/results')\nresults_dir.mkdir(parents=True, exist_ok=True)\n\nif 'results' in dir():\n    out = results_dir / 'eval_results.json'\n    with open(out, 'w') as f:\n        json.dump(results, f, indent=2, default=str)\n    print(f\"Results saved to: {out}\")\nelse:\n    print(\"No evaluation results yet — run Section 6 first.\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "code-download",
   "metadata": {
    "id": "code-download"
   },
   "source": [
    "# Download best model as zip\n",
    "import shutil\n",
    "from google.colab import files as colab_files\n",
    "\n",
    "model_dir = Path('/content/ai4ed-qg/models/topic/best_model')\n",
    "if model_dir.exists():\n",
    "    shutil.make_archive('/content/t5_topic_best_model', 'zip', model_dir)\n",
    "    colab_files.download('/content/t5_topic_best_model.zip')\n",
    "else:\n",
    "    print(\"Model not found — train first\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ]
}