{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.10.0"},
  "colab": {"provenance": []}
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "md-title",
   "metadata": {},
   "source": "# 01 â€” Data Pipeline\n\nRuns pipeline **stages 1â€“4** to produce training datasets for topic-controlled question generation.\n\n| Stage | Command | Output |\n|-------|---------|--------|\n| 1 â€” Convert | `pipe.convert()` | Flat JSON records from SQuAD / KhanQ |\n| 2 â€” Wikify | `pipe.wikify()` | Wikipedia entity annotations |\n| 3 â€” Topics | `pipe.topics()` | Best topic selected per QA pair |\n| 4 â€” Dataset | `pipe.dataset()` | Train/val/test CSVs |\n\nAll stages are **idempotent** â€” re-running a completed stage prints `[SKIP]` and returns immediately."
  },
  {
   "cell_type": "markdown",
   "id": "md-setup",
   "metadata": {},
   "source": "## Setup"
  },
  {
   "cell_type": "code",
   "id": "code-env",
   "metadata": {},
   "source": "import sys\nfrom pathlib import Path\n\nIN_COLAB = 'google.colab' in sys.modules\n\nif IN_COLAB:\n    # â”€â”€ Clone repository â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    # TODO: replace with your actual repository URL\n    REPO_URL = \"https://github.com/YOUR_ORG/YOUR_REPO.git\"\n    !git clone {REPO_URL} /content/ai4ed-qg -q\n    %cd /content/ai4ed-qg\n\n    # â”€â”€ Install dependencies â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    !pip install -q transformers sentencepiece requests tqdm pandas \\\n                    scikit-learn pyyaml python-dotenv\n\n    # â”€â”€ Mount Google Drive for persistent storage â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    # Data survives session restarts when saved to Drive.\n    from google.colab import drive\n    drive.mount('/content/drive')\n    DRIVE_DIR = Path('/content/drive/MyDrive/ai4ed_qg')\n    DRIVE_DIR.mkdir(parents=True, exist_ok=True)\n    print(f\"Drive mounted. Persistent storage: {DRIVE_DIR}\")\nelse:\n    # Running locally â€” assume cwd is repo root or notebooks/\n    DRIVE_DIR = None\n\nimport os\nproject_root = Path('/content/ai4ed-qg') if IN_COLAB else Path.cwd()\nif project_root.name == 'notebooks':\n    project_root = project_root.parent\nos.chdir(project_root)            # pipeline.yaml uses relative paths\nsys.path.insert(0, str(project_root))\n\nprint(f\"Project root : {project_root}\")\nprint(f\"Working dir  : {os.getcwd()}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "md-apikeys",
   "metadata": {},
   "source": "## API Keys\n\nThe Wikify stage calls the Wikifier.org API. Get a free key at https://wikifier.org/register.html.\n\nStore it in Colab Secrets (`ğŸ”‘` sidebar) under `WIKIFIER_API_KEY`, or set `WIKIFIER_API_KEY` in a local `.env` file."
  },
  {
   "cell_type": "code",
   "id": "code-apikeys",
   "metadata": {},
   "source": "if IN_COLAB:\n    from google.colab import userdata\n    import os\n    try:\n        os.environ['WIKIFIER_API_KEY'] = userdata.get('WIKIFIER_API_KEY')\n        print(\"Wikifier API key loaded from Colab Secrets\")\n    except Exception:\n        # Fallback: paste key directly\n        os.environ['WIKIFIER_API_KEY'] = \"\"  # â† paste your key here\n        print(\"WARNING: Set WIKIFIER_API_KEY above or add it to Colab Secrets\")\nelse:\n    try:\n        from dotenv import load_dotenv\n        load_dotenv()\n    except ImportError:\n        pass\n\nkey = os.environ.get('WIKIFIER_API_KEY', '')\nif key:\n    print(f\"WIKIFIER_API_KEY: {key[:6]}... (set)\")\nelse:\n    print(\"WIKIFIER_API_KEY not set â€” Stage 2 (wikify) will fail\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "md-init",
   "metadata": {},
   "source": "## Initialise Pipeline"
  },
  {
   "cell_type": "code",
   "id": "code-init",
   "metadata": {},
   "source": "from src.pipeline import Pipeline\n\npipe = Pipeline('config/pipeline.yaml')\npipe.status()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "md-rawdata",
   "metadata": {},
   "source": "## Raw Data\n\nThe convert stage expects these files in `data/raw/`:\n\n| File | Source |\n|------|--------|\n| `train-v1.1.json` | [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) |\n| `khanq.json` | [KhanQ dataset](https://github.com/topic-qg/khanq) |\n\nFor Colab, upload them to Drive and symlink below."
  },
  {
   "cell_type": "code",
   "id": "code-rawdata",
   "metadata": {},
   "source": "# â”€â”€ Colab only: copy raw files from Drive â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nimport shutil\nfrom pathlib import Path\n\nraw_dir = project_root / 'data' / 'raw'\nraw_dir.mkdir(parents=True, exist_ok=True)\n\nif IN_COLAB and DRIVE_DIR:\n    for fname in ('train-v1.1.json', 'khanq.json'):\n        src = DRIVE_DIR / 'raw' / fname\n        dst = raw_dir / fname\n        if src.exists() and not dst.exists():\n            shutil.copy(src, dst)\n            print(f\"Copied {fname} from Drive\")\n\nfor fname in ('train-v1.1.json', 'khanq.json'):\n    p = raw_dir / fname\n    status = f\"{p.stat().st_size / 1e6:.1f} MB\" if p.exists() else \"MISSING\"\n    print(f\"  {fname}: {status}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "md-stage1",
   "metadata": {},
   "source": "## Stage 1 â€” Convert\n\nConverts raw SQuAD and KhanQ JSON into flat records ready for wikification."
  },
  {
   "cell_type": "code",
   "id": "code-stage1",
   "metadata": {},
   "source": "pipe.convert(dataset='all')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "md-stage2",
   "metadata": {},
   "source": "## Stage 2 â€” Wikify\n\nAnnotates every text passage and question with Wikipedia entity links.\n\n> âš ï¸ **SQuAD is large (~90k texts).** Progress is saved every 50 items so it's safe to interrupt and resume. KhanQ (~1,034 texts) finishes in a few minutes.\n\nChoose `tool`:\n- `wikifier` â€” free, just needs `WIKIFIER_API_KEY`\n- `wat` â€” requires WAT token (`WAT_TOKEN` env var)"
  },
  {
   "cell_type": "code",
   "id": "code-stage2",
   "metadata": {},
   "source": "WIKIFY_TOOL = 'wikifier'  # 'wikifier' | 'wat'\n\n# Annotate KhanQ first (fast, ~few minutes)\npipe.wikify(dataset='khanq', tool=WIKIFY_TOOL, target='all')\n\n# Annotate SQuAD (slow, run across multiple sessions if needed)\n# pipe.wikify(dataset='squad', tool=WIKIFY_TOOL, target='all')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "code-sync-drive-wikify",
   "metadata": {},
   "source": "# â”€â”€ Colab: sync processed data to Drive so it survives session restart â”€â”€â”€â”€â”€â”€â”€â”€\nif IN_COLAB and DRIVE_DIR:\n    import shutil\n    src = project_root / 'data' / 'processed'\n    dst = DRIVE_DIR / 'processed'\n    shutil.copytree(src, dst, dirs_exist_ok=True)\n    print(f\"Synced data/processed/ â†’ {dst}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "md-stage3",
   "metadata": {},
   "source": "## Stage 3 â€” Topic Selection\n\nFor each QA pair, selects the best Wikipedia entity topic using:\n1. Intersection of entities annotated in both the text passage and the question\n2. Highest relevance score (`pageRank` for Wikifier, `rho` for WAT)"
  },
  {
   "cell_type": "code",
   "id": "code-stage3",
   "metadata": {},
   "source": "pipe.topics(dataset='khanq', tool=WIKIFY_TOOL)\n# pipe.topics(dataset='squad', tool=WIKIFY_TOOL)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "md-stage4",
   "metadata": {},
   "source": "## Stage 4 â€” Build Datasets\n\n| Mode | Entries | Use |\n|------|---------|-----|\n| `baseline` | ~full SQuAD | Baseline model (no topic signal) |\n| `mixsquad` | 10,000 | TopicQG model training |\n| `mixsquad2x` | ~20,000 | TopicQG2X (augmented) training |\n| `mixkhanq` | 653 | KhanQ evaluation set |"
  },
  {
   "cell_type": "code",
   "id": "code-stage4",
   "metadata": {},
   "source": "# KhanQ evaluation set â€” needed for the evaluation notebook\npipe.dataset(dataset='khanq', mode='mixkhanq', tool=WIKIFY_TOOL)\n\n# SQuAD training sets â€” needed for the training notebook\n# Uncomment after Stage 2 SQuAD wikification is complete:\n# pipe.dataset(dataset='squad', mode='baseline',   tool=WIKIFY_TOOL)\n# pipe.dataset(dataset='squad', mode='mixsquad',   tool=WIKIFY_TOOL)\n# pipe.dataset(dataset='squad', mode='mixsquad2x', tool=WIKIFY_TOOL)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "md-status",
   "metadata": {},
   "source": "## Final Status"
  },
  {
   "cell_type": "code",
   "id": "code-status",
   "metadata": {},
   "source": "pipe.status()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "code-sync-drive-final",
   "metadata": {},
   "source": "# â”€â”€ Colab: final sync of all data to Drive â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nif IN_COLAB and DRIVE_DIR:\n    import shutil\n    for subdir in ('processed', 'training'):\n        src = project_root / 'data' / subdir\n        if src.exists():\n            dst = DRIVE_DIR / subdir\n            shutil.copytree(src, dst, dirs_exist_ok=True)\n            print(f\"Synced data/{subdir}/ â†’ {dst}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "md-next",
   "metadata": {},
   "source": "## Next Steps\n\nProceed to **`02_distillation_training.ipynb`** to fine-tune T5 on the generated datasets."
  }
 ]
}
