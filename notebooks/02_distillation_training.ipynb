{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.10.0"},
  "colab": {"provenance": []}
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "md-title",
   "metadata": {},
   "source": "# 02 — Training\n\nFine-tunes a T5-small model on topic-controlled question generation.\n\n| Mode | Input format | Dataset | Paper name |\n|------|-------------|---------|------------|\n| `baseline` | `<topic> {t} <context> {text}` | SQuAD baseline | Baseline |\n| `topic` | `<topic> {t} <context> {text}` | MixSQuAD (10k) | TopicQG |\n| `topic2x` | `<topic> {t} <context> {text}` | MixSQuAD2X (20k) | TopicQG2X |\n\n> Prerequisite: run `01_data_generation.ipynb` first (stages 1–4)."
  },
  {
   "cell_type": "markdown",
   "id": "md-setup",
   "metadata": {},
   "source": "## Setup"
  },
  {
   "cell_type": "code",
   "id": "code-env",
   "metadata": {},
   "source": "import sys\nfrom pathlib import Path\n\nIN_COLAB = 'google.colab' in sys.modules\n\nif IN_COLAB:\n    REPO_URL = \"https://github.com/YOUR_ORG/YOUR_REPO.git\"  # TODO: set your URL\n    !git clone {REPO_URL} /content/ai4ed-qg -q\n    %cd /content/ai4ed-qg\n    !pip install -q torch transformers datasets accelerate sentencepiece pyyaml tqdm\n\n    from google.colab import drive\n    drive.mount('/content/drive')\n    DRIVE_DIR = Path('/content/drive/MyDrive/ai4ed_qg')\n\n    # Restore data from Drive\n    import shutil\n    for subdir in ('processed', 'training'):\n        src = DRIVE_DIR / subdir\n        dst = Path('/content/ai4ed-qg/data') / subdir\n        if src.exists():\n            shutil.copytree(src, dst, dirs_exist_ok=True)\n            print(f\"Restored data/{subdir}/ from Drive\")\nelse:\n    DRIVE_DIR = None\n\nimport os\nproject_root = Path('/content/ai4ed-qg') if IN_COLAB else Path.cwd()\nif project_root.name == 'notebooks':\n    project_root = project_root.parent\nos.chdir(project_root)\nsys.path.insert(0, str(project_root))\nprint(f\"Working dir: {os.getcwd()}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "code-gpu",
   "metadata": {},
   "source": "import torch\nprint(f\"PyTorch  : {torch.__version__}\")\nprint(f\"CUDA     : {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU      : {torch.cuda.get_device_name(0)}\")\n    mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n    print(f\"VRAM     : {mem:.1f} GB\")\n    if mem >= 35:\n        print(\"Tip: A100 detected — you can increase batch size to 128\")\n    elif mem >= 15:\n        print(\"Tip: V100/T4 — batch size 64 is fine\")\n    else:\n        print(\"Tip: small GPU — reduce batch size if OOM\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "md-init",
   "metadata": {},
   "source": "## Initialise Pipeline"
  },
  {
   "cell_type": "code",
   "id": "code-init",
   "metadata": {},
   "source": "from src.pipeline import Pipeline\n\npipe = Pipeline('config/pipeline.yaml')\npipe.status()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "md-config",
   "metadata": {},
   "source": "## Configuration\n\nDefaults are in `config/pipeline.yaml`. Override any value in the dict below."
  },
  {
   "cell_type": "code",
   "id": "code-config",
   "metadata": {},
   "source": "# Override training config (optional)\n# These mirror config/pipeline.yaml — edit pipeline.yaml to make permanent changes.\noverrides = {\n    # 'model_name': 'google-t5/t5-small',  # or 'google-t5/t5-base'\n    # 'batch':       64,\n    # 'lr':          1e-3,\n    # 'epochs':      50,\n    # 'max_input_len':  200,\n    # 'max_output_len': 45,\n}\nfor k, v in overrides.items():\n    setattr(pipe.config.training, k, v)\n\nt = pipe.config.training\nprint(f\"Model      : {t.model_name}\")\nprint(f\"Epochs     : {t.epochs}\")\nprint(f\"Batch size : {t.batch}\")\nprint(f\"LR         : {t.lr}\")\nprint(f\"Max input  : {t.max_input_len} tokens\")\nprint(f\"Max output : {t.max_output_len} tokens\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "md-train",
   "metadata": {},
   "source": "## Train Models\n\nTrain one or all three modes. Each saves to `models/{mode}/best_model/`.\n\n> Expected time on a T4 GPU: ~2–5 min/epoch × 50 epochs ≈ 2–4 hours per mode."
  },
  {
   "cell_type": "code",
   "id": "code-train-topic",
   "metadata": {},
   "source": "# ── TopicQG (main model — train this first) ───────────────────────────────────\nmodel_path = pipe.train(mode='topic', dataset='squad')\nprint(f\"\\nSaved to: {model_path}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "code-train-baseline",
   "metadata": {},
   "source": "# ── Baseline (context-only, no topic signal) ──────────────────────────────────\n# model_path = pipe.train(mode='baseline', dataset='squad')\n# print(f\"Saved to: {model_path}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "code-train-topic2x",
   "metadata": {},
   "source": "# ── TopicQG2X (doubled dataset, context order reversed) ──────────────────────\n# model_path = pipe.train(mode='topic2x', dataset='squad')\n# print(f\"Saved to: {model_path}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "md-test",
   "metadata": {},
   "source": "## Test Generation\n\nQuick sanity check — generate a question with the trained model."
  },
  {
   "cell_type": "code",
   "id": "code-test",
   "metadata": {},
   "source": "topic   = \"Photosynthesis\"\ncontext = (\n    \"Photosynthesis is a process used by plants and other organisms to convert \"\n    \"light energy into chemical energy that can be stored and used. \"\n    \"In plants, photosynthesis converts carbon dioxide and water into glucose \"\n    \"and oxygen using energy from sunlight. Chlorophyll, the green pigment in \"\n    \"leaves, is responsible for absorbing light energy.\"\n)\n\nquestion = pipe.generate(topic=topic, context=context, mode='topic')\nprint(f\"Topic   : {topic}\")\nprint(f\"Question: {question}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "md-save-drive",
   "metadata": {},
   "source": "## Save Model to Drive (Colab)"
  },
  {
   "cell_type": "code",
   "id": "code-save-drive",
   "metadata": {},
   "source": "if IN_COLAB and DRIVE_DIR:\n    import shutil\n    src = project_root / 'models'\n    dst = DRIVE_DIR / 'models'\n    dst.mkdir(parents=True, exist_ok=True)\n    shutil.copytree(src, dst, dirs_exist_ok=True)\n    print(f\"Models synced to {dst}\")\n\n    # Also available as zip download\n    shutil.make_archive('/content/models', 'zip', project_root / 'models')\n    from google.colab import files\n    files.download('/content/models.zip')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "md-next",
   "metadata": {},
   "source": "## Next Steps\n\nProceed to **`03_evaluation.ipynb`** to evaluate trained models against the paper's baselines."
  }
 ]
}
