{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.10.0"},
  "colab": {"provenance": []}
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "md-title",
   "metadata": {},
   "source": "# 04 — Concept Extraction & Targeted Question Generation\n\nUses the pipeline's Wikifier/WAT integration to extract Wikipedia concepts from any educational text, then generates targeted questions for each concept using the trained T5 model.\n\n**Workflow:**\n1. Wikify a passage — extract ranked Wikipedia entities\n2. Select top concepts by relevance score\n3. Generate a question per concept using `pipe.generate()`\n4. Export question bank as JSON / CSV\n\nThis is a lightweight alternative to a full knowledge graph — it produces a `(concept, context, question)` triple set directly usable for assessment or adaptive learning."
  },
  {
   "cell_type": "markdown",
   "id": "md-setup",
   "metadata": {},
   "source": "## Setup"
  },
  {
   "cell_type": "code",
   "id": "code-env",
   "metadata": {},
   "source": "import sys\nfrom pathlib import Path\n\nIN_COLAB = 'google.colab' in sys.modules\n\nif IN_COLAB:\n    REPO_URL = \"https://github.com/YOUR_ORG/YOUR_REPO.git\"  # TODO: set your URL\n    !git clone {REPO_URL} /content/ai4ed-qg -q\n    %cd /content/ai4ed-qg\n    !pip install -q torch transformers sentence-transformers sentencepiece \\\n                    requests pyyaml tqdm pandas python-dotenv\n\n    from google.colab import drive\n    drive.mount('/content/drive')\n    DRIVE_DIR = Path('/content/drive/MyDrive/ai4ed_qg')\n\n    # Restore trained model from Drive\n    import shutil\n    src = DRIVE_DIR / 'models'\n    if src.exists():\n        shutil.copytree(src, Path('/content/ai4ed-qg/models'), dirs_exist_ok=True)\n        print(\"Restored models/ from Drive\")\nelse:\n    DRIVE_DIR = None\n\nimport os\nproject_root = Path('/content/ai4ed-qg') if IN_COLAB else Path.cwd()\nif project_root.name == 'notebooks':\n    project_root = project_root.parent\nos.chdir(project_root)\nsys.path.insert(0, str(project_root))\nprint(f\"Working dir: {os.getcwd()}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "code-apikeys",
   "metadata": {},
   "source": "# Wikifier API key — https://wikifier.org/register.html\nif IN_COLAB:\n    from google.colab import userdata\n    try:\n        os.environ['WIKIFIER_API_KEY'] = userdata.get('WIKIFIER_API_KEY')\n        print(\"Wikifier key loaded from Colab Secrets\")\n    except Exception:\n        os.environ['WIKIFIER_API_KEY'] = \"\"  # ← paste your key here\nelse:\n    try:\n        from dotenv import load_dotenv\n        load_dotenv()\n    except ImportError:\n        pass\n\nkey = os.environ.get('WIKIFIER_API_KEY', '')\nprint(f\"WIKIFIER_API_KEY: {'set' if key else 'NOT SET — wikification will fail'}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "md-init",
   "metadata": {},
   "source": "## Initialise Pipeline"
  },
  {
   "cell_type": "code",
   "id": "code-init",
   "metadata": {},
   "source": "from src.pipeline import Pipeline\n\npipe = Pipeline('config/pipeline.yaml')\npipe.status()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "md-text",
   "metadata": {},
   "source": "## Define Educational Text\n\nPaste any educational passage below. The Wikifier will annotate it with Wikipedia concepts."
  },
  {
   "cell_type": "code",
   "id": "code-text",
   "metadata": {},
   "source": "# ── Edit this block with your own educational content ─────────────────────────\n\npassages = [\n    {\n        \"id\": \"chem-01\",\n        \"text\": (\n            \"Electronegativity is a measure of the tendency of an atom to attract \"\n            \"a bonding pair of electrons. The Pauling scale is the most commonly used. \"\n            \"Fluorine is the most electronegative element (4.0 on Pauling scale). \"\n            \"Electronegativity increases across a period and decreases down a group \"\n            \"in the periodic table. The difference in electronegativity between atoms \"\n            \"determines the polarity of a chemical bond.\"\n        ),\n    },\n    {\n        \"id\": \"bio-01\",\n        \"text\": (\n            \"Photosynthesis is a process by which plants, algae, and some bacteria \"\n            \"convert light energy into chemical energy stored as glucose. \"\n            \"It occurs in the chloroplasts, specifically using chlorophyll to absorb \"\n            \"sunlight. The overall reaction combines carbon dioxide and water to produce \"\n            \"glucose and oxygen. The light-dependent reactions occur in the thylakoid \"\n            \"membranes, while the Calvin cycle takes place in the stroma.\"\n        ),\n    },\n]\n\nprint(f\"Loaded {len(passages)} passage(s)\")\nfor p in passages:\n    print(f\"  [{p['id']}] {p['text'][:80]}...\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "md-wikify",
   "metadata": {},
   "source": "## Wikify Passages\n\nAnnotate each passage with Wikipedia entities using the Wikifier API."
  },
  {
   "cell_type": "code",
   "id": "code-wikify",
   "metadata": {},
   "source": "from src.wikification import get_wikifier\n\n# Construct the wikifier directly (no need to run the full pipeline)\nwikifier = get_wikifier('wikifier', pipe.config)\n\nannotated = []\nfor passage in passages:\n    annotations = wikifier.annotate(passage['text'])\n    annotated.append({\n        **passage,\n        'entities': annotations,  # list of {title, wiki_id, score, spot}\n    })\n    print(f\"[{passage['id']}] Found {len(annotations)} entities\")\n    for e in annotations[:5]:  # show top 5\n        print(f\"    {e['score']:6.4f}  {e['title']}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "md-topics",
   "metadata": {},
   "source": "## Select Top Concepts\n\nFilter entities to the top-N by relevance score (pageRank for Wikifier, rho for WAT)."
  },
  {
   "cell_type": "code",
   "id": "code-topics",
   "metadata": {},
   "source": "TOP_N = 5  # concepts per passage\n\nconcept_list = []  # (passage_id, text, topic, score)\n\nfor item in annotated:\n    # Entities are already sorted by score (descending)\n    top_entities = item['entities'][:TOP_N]\n    for e in top_entities:\n        concept_list.append({\n            'passage_id': item['id'],\n            'text':       item['text'],\n            'topic':      e['title'],\n            'score':      e['score'],\n        })\n\nprint(f\"Total concept-passage pairs: {len(concept_list)}\")\nfor c in concept_list:\n    print(f\"  [{c['passage_id']}]  {c['score']:.4f}  {c['topic']}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "md-generate",
   "metadata": {},
   "source": "## Generate Questions\n\nFor each concept, generate a question using the trained T5 model.\n\n> Requires `models/topic/best_model/` from `02_distillation_training.ipynb`. If not available, set `USE_GEMINI = True` to use Gemini instead."
  },
  {
   "cell_type": "code",
   "id": "code-generate",
   "metadata": {},
   "source": "from tqdm.notebook import tqdm\n\nUSE_GEMINI = False   # Set True to use Gemini instead of T5\nGEMINI_MODEL = 'gemini-2.5-flash'\n\nif USE_GEMINI:\n    from src.evaluation.models import GeminiBaseline\n    generator = GeminiBaseline(GEMINI_MODEL)\n    gen_fn = lambda topic, text: generator.generate_question(topic, text)\nelse:\n    gen_fn = lambda topic, text: pipe.generate(\n        topic=topic, context=text, mode='topic'\n    )\n\nquestion_bank = []\n\nfor concept in tqdm(concept_list, desc=\"Generating\"):\n    try:\n        question = gen_fn(concept['topic'], concept['text'])\n    except (FileNotFoundError, Exception) as exc:\n        question = f\"[ERROR: {exc}]\"\n\n    entry = {\n        **concept,\n        'question': question,\n    }\n    question_bank.append(entry)\n    print(f\"  [{concept['passage_id']}]  {concept['topic']:30s}  →  {question}\")\n\nprint(f\"\\nGenerated {len(question_bank)} questions\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "md-review",
   "metadata": {},
   "source": "## Review Question Bank"
  },
  {
   "cell_type": "code",
   "id": "code-review",
   "metadata": {},
   "source": "import pandas as pd\n\ndf = pd.DataFrame(question_bank)[['passage_id', 'topic', 'score', 'question']]\ndf['score'] = df['score'].round(4)\npd.set_option('display.max_colwidth', 80)\ndf",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "md-export",
   "metadata": {},
   "source": "## Export Question Bank"
  },
  {
   "cell_type": "code",
   "id": "code-export",
   "metadata": {},
   "source": "import json\nfrom datetime import datetime\n\nout_dir = project_root / 'results' / f'question_bank_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}'\nout_dir.mkdir(parents=True, exist_ok=True)\n\n# CSV\ncsv_path = out_dir / 'question_bank.csv'\ndf.to_csv(csv_path, index=False)\nprint(f\"CSV : {csv_path}\")\n\n# JSON (full, includes original text)\njson_path = out_dir / 'question_bank.json'\nwith open(json_path, 'w', encoding='utf-8') as f:\n    json.dump(question_bank, f, indent=2, ensure_ascii=False)\nprint(f\"JSON: {json_path}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "code-download",
   "metadata": {},
   "source": "# Colab: download the CSV\nif IN_COLAB:\n    from google.colab import files\n    files.download(str(csv_path))",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "md-next",
   "metadata": {},
   "source": "## Next Steps\n\n- **Scale up**: Pass a larger document set in the `passages` list, or load from a file\n- **Use WAT**: Change `get_wikifier('wikifier', ...)` to `get_wikifier('wat', ...)` for higher-quality entity linking (requires WAT token)\n- **Zero-shot comparison**: Set `USE_GEMINI = True` to compare T5 vs Gemini question quality\n- **Filter by score**: Increase the score threshold to keep only high-confidence concepts"
  }
 ]
}
