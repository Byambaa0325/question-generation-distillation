# Pipeline configuration for topic-controlled question generation.
# All paths are relative to the project root.

paths:
  raw:       data/raw
  processed: data/processed
  training:  data/training
  models:    models
  results:   results

wikification:
  tool:       wikifier    # wikifier | wat
  top_n:      5
  chunk_size: 650
  save_every: 50
  # Wikifier-specific
  df_ignore:    200
  words_ignore: 200
  # WAT-specific
  wat_lang: en

dataset:
  max_tokens:        510
  train_ratio:       0.70
  val_ratio:         0.15
  test_ratio:        0.15
  mix_samples:       10000
  khanq_mix_samples: 653
  seed:              42

training:
  model_name:         google-t5/t5-small
  batch:              64
  lr:                 1.0e-3
  epochs:             50
  max_input_len:      200
  max_output_len:     45
  warmup_steps:            0
  weight_decay:            0.01  # L2 regularisation; helps prevent overfitting
  early_stopping_patience: 5     # stop if val loss doesn't improve for N epochs; 0 = off
  fp16:               false       # set true for T4/V100 in Colab
  bf16:               false       # set true for A100/H100 in Colab
  grad_accum:         1           # increase to simulate larger batch without more VRAM
  dataloader_workers: 0           # 0 = safe on Windows; set 2-4 on Linux/Colab
  special_tokens:
    - "<sep>"
    - "<space>"

evaluation:
  max_samples:        null   # null = all samples
  compute_wikisemrel: false
  num_beams:          10
  num_return_sequences: 8
  # Non-auto-discoverable models included when models='all' is used.
  # Ollama models are auto-discovered from the running server â€” no need to list them here.
  # Gemini models require GOOGLE_API_KEY env var.
  zero_shot_models:
    - gemini:gemini-2.5-pro
    - gemini:gemini-2.5-flash
    - gemini:gemini-2.5-flash-lite
